[
  {
    "title": "UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme\n  Active-Site Knowledge",
    "authors": "Chenao Li, Shuo Yan, Enyan Dai",
    "abstract": "  Enzyme-catalyzed protein cleavage is essential for many biological functions.\nAccurate prediction of cleavage sites can facilitate various applications such\nas drug development, enzyme design, and a deeper understanding of biological\nmechanisms. However, most existing models are restricted to an individual\nenzyme, which neglects shared knowledge of enzymes and fails generalize to\nnovel enzymes. Thus, we introduce a unified protein cleavage site predictor\nnamed UniZyme, which can generalize across diverse enzymes. To enhance the\nenzyme encoding for the protein cleavage site prediction, UniZyme employs a\nnovel biochemically-informed model architecture along with active-site\nknowledge of proteolytic enzymes. Extensive experiments demonstrate that\nUniZyme achieves high accuracy in predicting cleavage sites across a range of\nproteolytic enzymes, including unseen enzymes. The code is available in\nhttps://anonymous.4open.science/r/UniZyme-4A67.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.06914v2",
    "published_date": "2025-02-10T09:46:26Z",
    "keywords": []
  },
  {
    "title": "Foliar Uptake of Biocides: Statistical Assessment of Compartmental and\n  Diffusion-Based Models",
    "authors": "Enrico Sangoi, Federica Cattani, Faheem Padia, Federico Galvanin",
    "abstract": "  The global population increase leads to a high food demand, and to reach this\ntarget products such as pesticides are needed to protect the crops. Research is\nfocusing on the development of new products that can be less harmful to the\nenvironment, and mathematical models are tools that can help to understand the\nmechanism of uptake of pesticides and then guide in the product development\nphase. This paper applies a systematic methodology to model the foliar uptake\nof pesticides, to take into account the uncertainties in the experimental data\nand in the model structure. A comparison between different models is conducted,\nfocusing on the identifiability of model parameters through dynamic sensitivity\nprofiles and correlation analysis. Lastly, data augmentation studies are\nconducted to exploit the model for the design of experiments and to provide a\npractical support to future experimental campaigns, paving the way for further\napplication of model-based design of experiments techniques in the context of\nfoliar uptake.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.08533v1",
    "published_date": "2025-02-12T16:14:07Z",
    "keywords": []
  },
  {
    "title": "A diffusion MRI model for random walks confined on cylindrical surfaces:\n  Towards non-invasive quantification of myelin sheath radius",
    "authors": "Erick J Canales-Rodríguez, Chantal M. W. Tax, Elda Fischi-Gomez, Derek K. Jones, Jean-Philippe Thiran, Jonathan Rafael-Patiño",
    "abstract": "  Quantifying the myelin sheath radius of myelinated axons in vivo is important\nfor understanding, diagnosing, and monitoring various neurological disorders.\nDespite advancements in diffusion MRI (dMRI) microstructure techniques, there\nare currently no models specifically designed to estimate myelin sheath radii.\nThis proof-of-concept theoretical study presents two novel dMRI models that\ncharacterize the signal from water diffusion confined to cylindrical surfaces,\napproximating myelin water diffusion. We derive their spherical mean signals,\neliminating fiber orientation and dispersion effects for convenience. These\nmodels are further extended to account for multiple concentric cylinders,\nmimicking the layered structure of myelin. Additionally, we introduce a method\nto convert histological distributions of axonal inner radii from the literature\ninto myelin sheath radius distributions. We also derive analytical expressions\nto estimate the effective myelin sheath radius expected from these\ndistributions. Monte Carlo (MC) simulations conducted in cylindrical and spiral\ngeometries validate the models. These simulations demonstrate agreement with\nanalytical predictions. Furthermore, we observe significant correlations\nbetween the effective radii derived from histological distributions and those\nobtained by fitting the dMRI signal to a single-cylinder model. These models\nmay be integrated with existing multi-compartment dMRI techniques, opening the\ndoor to non-invasive in vivo assessments of myelin sheath radii. Such\nassessments would require MRI scanners equipped with strong diffusion\ngradients, allowing measurements with short echo times. Further work is\nrequired to validate the technique with real dMRI data and histological\nmeasurements.\n",
    "pdf_url": "http://arxiv.org/pdf/2410.18842v2",
    "published_date": "2024-10-24T15:26:10Z",
    "keywords": []
  },
  {
    "title": "Impact of Electric Spatially Discordant Alternans on Cardiac Magnetic\n  Field",
    "authors": "Martina Nicoletti, Anna Crispino, Alessandro Loppini, Alessio Gizzi, Letizia Chiodo, Christian Cherubini, Simonetta Filippi",
    "abstract": "  Spatially discordant alternans (SDA) play a crucial role in cardiac\narrhythmogenesis by creating steep repolarization gradients facilitating\nconduction block and reentry. While traditionally studied using electrical\nindicators, this work provides a novel perspective by characterizing SDA\nthrough their magnetic field signatures. Using a one-dimensional cardiac fiber\nmodel, we demonstrate that magnetic field measurements effectively detect SDA\nand temperature dependent changes in cardiac action potentials, offering a\nnon-invasive alternative to conventional electrophysiological metrics. Our\nresults reveal that the spatial organization of SDA is mirrored in the magnetic\nfield distribution, with SDA nodes clearly identifiable via spatial mapping.\nNotably, magnetic restitution curves exhibit a distinct pattern from APD-based\nindicators, closely following the dynamics of the action potential upstroke.\nThese findings establish the cardiac magnetic field as a powerful diagnostic\ntool for detecting SDA, opening new avenues for biomagnetic monitoring of\narrhythmic risk.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.08480v1",
    "published_date": "2025-02-12T15:14:25Z",
    "keywords": []
  },
  {
    "title": "Ordinal Characterization of Similarity Judgments",
    "authors": "Jonathan D. Victor, Guillermo Aguilar, Suniyya A. Waraich",
    "abstract": "  Characterizing judgments of similarity within a perceptual or semantic\ndomain, and making inferences about the underlying structure of this domain\nfrom these judgments, has an increasingly important role in cognitive and\nsystems neuroscience. We present a new framework for this purpose that makes\nlimited assumptions about how perceptual distances are converted into\nsimilarity judgments. The approach starts from a dataset of empirical judgments\nof relative similarities: the fraction of times that a subject chooses one of\ntwo comparison stimuli to be more similar to a reference stimulus. These\nempirical judgments provide Bayesian estimates of underling choice\nprobabilities. From these estimates, we derive indices that characterize the\nset of judgments in three ways: compatibility with a symmetric dis-similarity,\ncompatibility with an ultrametric space, and compatibility with an additive\ntree. Each of the indices is derived from rank-order relationships among the\nchoice probabilities that, as we show, are necessary and sufficient for local\nconsistency with the three respective characteristics. We illustrate this\napproach with simulations and example psychophysical datasets of dis-similarity\njudgments in several visual domains and provide code that implements the\nanalyses at https://github.com/jvlab/simrank.\n",
    "pdf_url": "http://arxiv.org/pdf/2310.07543v3",
    "published_date": "2023-10-11T14:47:51Z",
    "keywords": []
  },
  {
    "title": "Unbiased and Error-Detecting Combinatorial Pooling Experiments with\n  Balanced Constant-Weight Gray Codes for Consecutive Positives Detection",
    "authors": "Guanchen He, Vasilisa A. Kovaleva, Carl Barton, Paul G. Thomas, Mikhail V. Pogorelyy, Hannah V. Meyer, Qin Huang",
    "abstract": "  Combinatorial pooling schemes have enabled the measurement of thousands of\nexperiments in a small number of reactions. This efficiency is achieved by\ndistributing the items to be measured across multiple reaction units called\npools. However, current methods for the design of pooling schemes do not\nadequately address the need for balanced item distribution across pools, a\nproperty particularly important for biological applications. Here, we introduce\nbalanced constant-weight Gray codes for detecting consecutive positives\n(DCP-CWGCs) for the efficient construction of combinatorial pooling schemes.\nBalanced DCP-CWGCs ensure uniform item distribution across pools, allow for the\nidentification of consecutive positive items such as overlapping biological\nsequences, and enable error detection by keeping the number of tests on\nindividual and consecutive positive items constant. For the efficient\nconstruction of balanced DCP-CWGCs, we have released an open-source python\npackage codePub, with implementations of the two core algorithms: a\nbranch-and-bound algorithm (BBA) and a recursive combination with BBA (rcBBA).\nSimulations using codePub show that our algorithms can construct long, balanced\nDCP-CWGCs that allow for error detection in tractable runtime.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.08214v1",
    "published_date": "2025-02-12T08:48:58Z",
    "keywords": []
  },
  {
    "title": "Global Deep Forecasting with Patient-Specific Pharmacokinetics",
    "authors": "Willa Potosnak, Cristian Challu, Kin G. Olivares, Keith A. Dufendach, Artur Dubrawski",
    "abstract": "  Forecasting healthcare time series data is vital for early detection of\nadverse outcomes and patient monitoring. However, it can be challenging in\npractice due to variable medication administration and unique pharmacokinetic\n(PK) properties of each patient. To address these challenges, we propose a\nnovel hybrid global-local architecture and a PK encoder that informs deep\nlearning models of patient-specific treatment effects. We showcase the efficacy\nof our approach in achieving significant accuracy gains in a blood glucose\nforecasting task using both realistically simulated and real-world data. Our PK\nencoder surpasses baselines by up to 16.4% on simulated data and 4.9% on\nreal-world data for individual patients during critical events of severely high\nand low glucose levels. Furthermore, our proposed hybrid global-local\narchitecture outperforms patient-specific PK models by 15.8%, on average.\n",
    "pdf_url": "http://arxiv.org/pdf/2309.13135v8",
    "published_date": "2023-09-22T18:43:41Z",
    "keywords": []
  },
  {
    "title": "An affordable, wearable, fiber-free pulsed-mode diffuse speckle contrast\n  flowmetry (PM-DSCF) sensor for noninvasive measurements of deep cerebral\n  blood flow",
    "authors": "Chaebeom Yeo, Xuhui Liu, Mehrana Mohtasebi, Faezeh Akbari, Faraneh Fathi, Guoqiang Yu",
    "abstract": "  Significance: Measuring cerebral blood flow (CBF) is crucial for diagnosing\nvarious cerebral diseases. An affordable, wearable, and fiber-free\ncontinuous-wave speckle contrast flowmetry (CW-DSCF) technique has been\ndeveloped for continuous monitoring of CBF variations. However, its application\nin adult humans is limited by shallow tissue penetration. Aim: To develop an\ninnovative pulse-mode DSCF (PM-DSCF) system for continuous monitoring of CBF\nvariations in adult humans. Approach: The PM-DSCF utilizes an 808 nm laser\ndiode and a small NanEye camera to capture diffuse laser speckle fluctuations\ncaused by red blood cell movement in the brain (i.e., CBF). Operating in\nshort-pulse mode (duty cycle < 5%), the system maximizes peak pulse light power\nfor deeper tissue penetration, while ensuring that the average power density\nremains within ANSI safety standards for skin exposure. The PM-DSCF was\nevaluated on tissue-simulating phantoms and in adult humans. Results: The\nmaximum effective source-detector distance increased from 15 mm (CW-DSCF) to 35\nmm (PM-DSCF). The PM-DSCF successfully detected CBF variations in adult brains\nduring head-up-tilting experiments, consistent with physiological expectations.\nConclusions: Switching from CW mode to PM mode significantly increases the\nmaximum tissue penetration depth from ~7.5 mm (CW-DSCF) to ~17.5 mm (PM-DSCF),\nenabling successful CBF measurements in adult humans.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.08000v1",
    "published_date": "2025-02-11T22:48:39Z",
    "keywords": []
  },
  {
    "title": "New tools for comparing classical and neural ODE models for tumor growth",
    "authors": "Anthony D. Blaom, Samuel Okon",
    "abstract": "  A new computational tool TumorGrowth.jl for modeling tumor growth is\nintroduced. The tool allows the comparison of standard textbook models, such as\nGeneral Bertalanffy and Gompertz, with some newer models, including, for the\nfirst time, neural ODE models. As an application, we revisit a human meta-study\nof non-small cell lung cancer and bladder cancer lesions, in patients\nundergoing two different treatment options, to determine if previously reported\nperformance differences are statistically significant, and if newer, more\ncomplex models perform any better. In a population of examples with at least\nfour time-volume measurements available for calibration, and an average of\nabout 6.3, our main conclusion is that the General Bertalanffy model has\nsuperior performance, on average. However, where more measurements are\navailable, we argue that more complex models, capable of capturing rebound and\nrelapse behavior, may be better choices.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.07964v1",
    "published_date": "2025-02-11T21:21:28Z",
    "keywords": [
      "cancer"
    ]
  },
  {
    "title": "Design of an Automated Ethanol Vapor Generating System for Alcohol Use\n  Disorder(AUD) Animal Studies",
    "authors": "Alexander Pozhitkov, Douglas Ramsay, Peter A Noble",
    "abstract": "  Alcohol Use Disorder (AUD) is a prevalent addictive disorder affecting an\nestimated 29.5 million Americans. It is characterized by impaired control over\nalcohol consumption despite negative consequences. The number of diagnostic\ncriteria met by an individual typically determines the severity of AUD.\nResearch into AUD focuses on understanding individual susceptibility\ndifferences and developing preventive strategies. Alcohol vapor inhalation has\nemerged as a promising method for pathophysiological investigations in animals,\nallowing researchers to control the dose and duration of alcohol exposure. This\napproach is crucial for studying the escalation of voluntary alcohol-drinking\nbehavior. Current commercial systems for alcohol vapor generation have\nlimitations, including combustion risks and the need to adjust multiple\nparameters. Other methods, like bubbling or blow-over evaporation, face\nchallenges in maintaining equilibrium and avoiding aerosolization. To address\nthese issues, a new type of ethanol vapor generating system is proposed that\nrelies solely on temperature control, creating a vacuum into which ethanol\nevaporates under thermodynamic control. This approach eliminates the need to\nadjust multiple parameters and offers improved accuracy and precision in vapor\ndose delivery. We validated the system as anticipated, achieving stable ethanol\nvapor after a few priming cycles. Using a 1.2 L cylinder, we obtained\napproximately 3.6 L of saturated vapor/air mix in 1 minute. Gravimetric results\nshowed that each cycle produced about 100 mg/L or ~10,000 ppm vapor-to-air\nmixture. The intended use of the ethanol vapor generator is to provide a\nconcentrated ethanol vapor / air mixture to be further diluted before\ndelivering to the animals.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.07860v1",
    "published_date": "2025-02-11T17:12:30Z",
    "keywords": []
  },
  {
    "title": "Topological analysis of brain dynamical signals indicates signatures of\n  seizure susceptibility",
    "authors": "Maxime Lucas, Damien Francois, Laurent Mombaerts, Cristina Donato, Alexander Skupin, Daniele Proverbio",
    "abstract": "  Epilepsy is known to drastically alter brain dynamics during seizures (ictal\nperiods), but its effects on background (non-ictal) brain dynamics remain\npoorly understood. To investigate this, we analyzed an in-house dataset of\nbrain activity recordings from epileptic zebrafish, focusing on two controlled\ngenetic conditions across two fishlines. After using machine learning to\nsegment and label recordings, we applied time-delay embedding and Persistent\nHomology -- a noise-robust method from Topological Data Analysis (TDA) -- to\nuncover topological patterns in brain activity. We find that ictal and\nnon-ictal periods can be distinguished based on the topology of their dynamics,\nindependent of genetic condition or fishline, which validates our approach.\nRemarkably, within a single wild-type fishline, we identified topological\ndifferences in non-ictal periods between seizure-prone and seizure-free\nindividuals. These findings suggest that epilepsy leaves detectable topological\nsignatures in brain dynamics even outside of ictal periods. Overall, this study\ndemonstrates the utility of TDA as a quantitative framework to screen for\ntopological markers of epileptic susceptibility, with potential applications\nacross species.\n",
    "pdf_url": "http://arxiv.org/pdf/2412.01911v3",
    "published_date": "2024-12-02T19:02:24Z",
    "keywords": []
  },
  {
    "title": "Measuring Fitness and Importance of Species in Food Webs",
    "authors": "Emanuele Calò, Giordano De Marzo, Vito D. P. Servedio",
    "abstract": "  Ecosystems face intensifying threats from climate change, overexploitation,\nand other human pressures, emphasizing the urgent need to identify keystone\nspecies and vulnerable ones. While established network-based measures often\nrely on a single metric to quantify a species' relevance, they overlook how\norganisms can be both carbon providers and consumers, thus playing a dual role\nin food webs. Here, we introduce a novel approach that assigns each species two\ncomplementary scores -- an importance measure quantifying their centrality as\ncarbon source and a fitness measure capturing their vulnerability. We show that\nspecies with high importance are more likely to trigger co-extinctions upon\nremoval, while high-fitness species typically endure until later stages of\ncollapse, in line with their broader prey ranges. On the other hand, low\nfitness species are the most vulnerable and susceptible to extinctions. Tested\non multiple food webs, our method outperforms traditional degree-based analyses\nand competes effectively with eigenvector-based approaches, while also\nproviding additional insights. Relying solely on interaction data, the approach\nis scalable and avoids reliance on expert-driven classifications, offering a\ncost-effective tool for prioritizing conservation efforts.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.07614v1",
    "published_date": "2025-02-11T15:05:21Z",
    "keywords": []
  },
  {
    "title": "Numerical Study on Human Brain Cortical Electrostimulation Assessment\n  During Uniform Magnetic Field Exposure at Intermediate Frequencies",
    "authors": "Jose Gomez-Tames, Thomas Tarnaud, Wout Joseph, Emmeric Tanghe",
    "abstract": "  Objectives: Permissible limits have been established by international\nguidelines and standards for human protection to electromagnetic field exposure\nto prevent adverse health effects stemming from electrostimulation in the most\nsensitive body part. That is the peripheral nervous system (PNS) in the\nintermediate frequency range (300 Hz to 100 kHz) and the central nervous system\n(CNS) at lower frequencies. However, there is a need to reevaluate protection\nlimits against CNS electrostimulation in the intermediate frequency range,\nconsidering the importance of brain tissues during electromagnetic head\nexposure. This study aims to derive the level of CNS cortical stimulation to\nevaluate compliance with existing protection limits. Method: Multi-scale\ncomputation modelling was used to evaluate neuron stimulation thresholds by\nintegrating individual neurons into realistic anatomical head models. Five\ndifferent excitable membrane models within the motor cortex were examined\nacross three human head models, providing the most comprehensive and extensive\nevaluation to date. Results: Current protection limits are confirmed as\nconservative, with non-compliance observed in only 0.02% and 2.4% of axons\nunder clamped and sealed boundary conditions, respectively. The study\nhighlights significant intersubject variability (up to 600% mean threshold) and\nclarifies the influence of neural excitation models on permissible level\nassessments. Conclusion: Current electric field limits are conservative for CNS\nelectrostimulation in the intermediate frequencies range, but the margin of\nsafety decreases at higher frequencies, warranting further evaluation. Impact:\nThe findings and methodology contribute to the rationale and provide valuable\ninsights for revising electromagnetic safety exposure guidelines.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.07565v1",
    "published_date": "2025-02-11T14:05:04Z",
    "keywords": []
  },
  {
    "title": "Compound Mask for Divergent Wave Imaging in Medical Ultrasound",
    "authors": "Zahraa Alzein, Marco Crocco, Daniele D. Caviglia",
    "abstract": "  Divergent wave imaging with coherent compounding allows obtaining broad field\nof view and higher frame rate with respect to line by line insonification.\nHowever, the spatial and contrast resolution crucially depends on the weights\napplied in the compound phase, whose optimization is often cumbersome and based\non trial and error. This study addresses these limitations by introducing a\nclosed-form approach that maps the transmit apodization weights used in\nsynthetic aperture imaging into the compound mask applied to divergent wave\nimaging. The approach draws inspiration from a successful technique developed\nfor plane wave imaging, leveraging synthetic aperture imaging as a reference\ndue to its superior image quality. It works for both linear and convex\ngeometries and arbitrary spatial arrangements of virtual sources generating\ndivergent waves. The approach has been validated through simulated data using\nboth linear and convex probes, demonstrating that the Full Width at Half\nMaximum (FWHM) in Divergent Wave Linear Array (DWLA) increased by 7.5% at 20 mm\nand 9% at 30 mm compared to Synthetic Aperture Linear Array (SALA). For\nDivergent Wave Convex Array (DWCA), the increase was 1.64% at 20 mm and 26.56%\nat 30 mm compared to Synthetic Aperture Convex Array (SACA), witnessing the\nmethod's effectiveness.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.07453v1",
    "published_date": "2025-02-11T10:53:36Z",
    "keywords": []
  },
  {
    "title": "Supervised contrastive learning for cell stage classification of animal\n  embryos",
    "authors": "Yasmine Hachani, Patrick Bouthemy, Elisa Fromont, Sylvie Ruffini, Ludivine Laffont, Alline de Paula Reis",
    "abstract": "  Video microscopy, when combined with machine learning, offers a promising\napproach for studying the early development of in vitro produced (IVP) embryos.\nHowever, manually annotating developmental events, and more specifically cell\ndivisions, is time-consuming for a biologist and cannot scale up for practical\napplications. We aim to automatically classify the cell stages of embryos from\n2D time-lapse microscopy videos with a deep learning approach. We focus on the\nanalysis of bovine embryonic development using video microscopy, as we are\nprimarily interested in the application of cattle breeding, and we have created\na Bovine Embryos Cell Stages (ECS) dataset. The challenges are three-fold: (1)\nlow-quality images and bovine dark cells that make the identification of cell\nstages difficult, (2) class ambiguity at the boundaries of developmental\nstages, and (3) imbalanced data distribution. To address these challenges, we\nintroduce CLEmbryo, a novel method that leverages supervised contrastive\nlearning combined with focal loss for training, and the lightweight 3D neural\nnetwork CSN-50 as an encoder. We also show that our method generalizes well.\nCLEmbryo outperforms state-of-the-art methods on both our Bovine ECS dataset\nand the publicly available NYU Mouse Embryos dataset.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.07360v1",
    "published_date": "2025-02-11T08:30:25Z",
    "keywords": []
  },
  {
    "title": "Generation of Drug-Induced Cardiac Reactions towards Virtual Clinical\n  Trials",
    "authors": "Qian Shao, Bang Du, Zepeng Li, Qiyuan Chen, Hongxia Xu, Jimeng Sun, Jian Wu, Jintai Chen",
    "abstract": "  Clinical trials are pivotal in cardiac drug development, yet they often fail\ndue to inadequate efficacy and unexpected safety issues, leading to significant\nfinancial losses. Using in-silico trials to replace a part of physical clinical\ntrials, e.g., leveraging advanced generative models to generate drug-influenced\nelectrocardiograms (ECGs), seems an effective method to reduce financial risk\nand potential harm to trial participants. While existing generative models have\ndemonstrated progress in ECG generation, they fall short in modeling drug\nreactions due to limited fidelity and inability to capture individualized drug\nresponse patterns. In this paper, we propose a Drug-Aware Diffusion Model\n(DADM), which could simulate individualized drug reactions while ensuring\nfidelity. To ensure fidelity, we construct a set of ordinary differential\nequations to provide external physical knowledge (EPK) of the realistic ECG\nmorphology. The EPK is used to adaptively constrain the morphology of the\ngenerated ECGs through a dynamic cross-attention (DCA) mechanism. Furthermore,\nwe propose an extension of ControlNet to incorporate demographic and drug data,\nsimulating individual drug reactions. We compare DADM with the other eight\nstate-of-the-art ECG generative models on two real-world databases covering 8\ntypes of drug regimens. The results demonstrate that DADM can more accurately\nsimulate drug-induced changes in ECGs, improving the accuracy by at least 5.79%\nand recall by 8%.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.07297v1",
    "published_date": "2025-02-11T06:50:33Z",
    "keywords": []
  },
  {
    "title": "ADEPT: A Noninvasive Method for Determining Elastic Parameters of Valve\n  Tissue",
    "authors": "Wensi Wu, Mitchell Daneker, Christian Herz, Hannah Dewey, Jeffrey A. Weiss, Alison M. Pouch, Lu Lu, Matthew A. Jolley",
    "abstract": "  Computer simulation of \"virtual interventions\" may inform optimal valve\nrepair for a given patient prior to intervention. However, the paucity of\nnoninvasive methods to determine in vivo mechanical parameters of valves limits\nthe accuracy of computer prediction and their clinical application. To address\nthis, we propose ADEPT: A noninvasive method for Determining Elastic Parameters\nof valve Tissue. In this work, we demonstrated its application to the tricuspid\nvalve of a child. We first tracked valve displacements from open to closed\nframes within a 3D echocardiogram time sequence using image registration.\nPhysics-informed neural networks were subsequently applied to estimate the\nnonlinear mechanical properties from first principles and reference\ndisplacements. The simulated model using these patient-specific parameters\nclosely aligned with the reference image segmentation, achieving a mean\nsymmetric distance of less than 1 mm. Our approach doubled the accuracy of the\nsimulated model compared to the generic parameters reported in the literature.\n",
    "pdf_url": "http://arxiv.org/pdf/2409.19081v2",
    "published_date": "2024-09-27T18:28:22Z",
    "keywords": []
  },
  {
    "title": "Advancing Precision Oncology Through Modeling of Longitudinal and\n  Multimodal Data",
    "authors": "Luoting Zhuang, Stephen H. Park, Steven J. Skates, Ashley E. Prosper, Denise R. Aberle, William Hsu",
    "abstract": "  Cancer evolves continuously over time through a complex interplay of genetic,\nepigenetic, microenvironmental, and phenotypic changes. This dynamic behavior\ndrives uncontrolled cell growth, metastasis, immune evasion, and therapy\nresistance, posing challenges for effective monitoring and treatment. However,\ntoday's data-driven research in oncology has primarily focused on\ncross-sectional analysis using data from a single modality, limiting the\nability to fully characterize and interpret the disease's dynamic\nheterogeneity. Advances in multiscale data collection and computational methods\nnow enable the discovery of longitudinal multimodal biomarkers for precision\noncology. Longitudinal data reveal patterns of disease progression and\ntreatment response that are not evident from single-timepoint data, enabling\ntimely abnormality detection and dynamic treatment adaptation. Multimodal data\nintegration offers complementary information from diverse sources for more\nprecise risk assessment and targeting of cancer therapy. In this review, we\nsurvey methods of longitudinal and multimodal modeling, highlighting their\nsynergy in providing multifaceted insights for personalized care tailored to\nthe unique characteristics of a patient's cancer. We summarize the current\nchallenges and future directions of longitudinal multimodal analysis in\nadvancing precision oncology.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.07836v1",
    "published_date": "2025-02-11T01:44:51Z",
    "keywords": [
      "cancer"
    ]
  },
  {
    "title": "Fourier-enhanced Neural Networks For Systems Biology Applications",
    "authors": "Enze Xu, Minghan Chen",
    "abstract": "  In the field of systems biology, differential equations are commonly used to\nmodel biological systems, but solving them for large-scale and complex systems\ncan be computationally expensive. Recently, the integration of machine learning\nand mathematical modeling has offered new opportunities for scientific\ndiscoveries in biology and health. The emerging physics-informed neural network\n(PINN) has been proposed as a solution to this problem. However, PINN can be\ncomputationally expensive and unreliable for complex biological systems. To\naddress these issues, we propose the Fourier-enhanced Neural Networks for\nsystems biology (SB-FNN). SB-FNN uses an embedded Fourier neural network with\nan adaptive activation function and a cyclic penalty function to optimize the\nprediction of biological dynamics, particularly for biological systems that\nexhibit oscillatory patterns. Experimental results demonstrate that SB-FNN\nachieves better performance and is more efficient than PINN for handling\ncomplex biological models. Experimental results on cellular and population\nmodels demonstrate that SB-FNN outperforms PINN in both accuracy and\nefficiency, making it a promising alternative approach for handling complex\nbiological models. The proposed method achieved better performance on six\nbiological models and is expected to replace PINN as the most advanced method\nin systems biology.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.07129v1",
    "published_date": "2025-02-10T23:48:10Z",
    "keywords": []
  },
  {
    "title": "Interpreting artificial neural networks to detect genome-wide\n  association signals for complex traits",
    "authors": "Burak Yelmen, Maris Alver, Merve Nur Güler, Estonian Biobank Research Team, Flora Jay, Lili Milani",
    "abstract": "  Investigating the genetic architecture of complex diseases is challenging due\nto the multifactorial and interactive landscape of genomic and environmental\ninfluences. Although genome-wide association studies (GWAS) have identified\nthousands of variants for multiple complex traits, conventional statistical\napproaches can be limited by simplified assumptions such as linearity and lack\nof epistasis in models. In this work, we trained artificial neural networks to\npredict complex traits using both simulated and real genotype-phenotype\ndatasets. We extracted feature importance scores via different post hoc\ninterpretability methods to identify potentially associated loci (PAL) for the\ntarget phenotype and devised an approach for obtaining p-values for the\ndetected PAL. Simulations with various parameters demonstrated that associated\nloci can be detected with good precision using strict selection criteria. By\napplying our approach to the schizophrenia cohort in the Estonian Biobank, we\ndetected multiple loci associated with this highly polygenic and heritable\ndisorder. There was significant concordance between PAL and loci previously\nassociated with schizophrenia and bipolar disorder, with enrichment analyses of\ngenes within the identified PAL predominantly highlighting terms related to\nbrain morphology and function. With advancements in model optimization and\nuncertainty quantification, artificial neural networks have the potential to\nenhance the identification of genomic loci associated with complex diseases,\noffering a more comprehensive approach for GWAS and serving as initial\nscreening tools for subsequent functional studies.\n",
    "pdf_url": "http://arxiv.org/pdf/2407.18811v2",
    "published_date": "2024-07-26T15:20:42Z",
    "keywords": []
  },
  {
    "title": "Efficient Spatial Estimation of Perceptual Thresholds for Retinal\n  Implants via Gaussian Process Regression",
    "authors": "Roksana Sadeghi, Michael Beyeler",
    "abstract": "  Retinal prostheses restore vision by electrically stimulating surviving\nneurons, but calibrating perceptual thresholds - the minimum stimulus intensity\nrequired for perception - remains a time-intensive challenge, especially for\nhigh-electrode-count devices. Since neighboring electrodes exhibit spatial\ncorrelations, we propose a Gaussian Process Regression (GPR) framework to\npredict thresholds at unsampled locations while leveraging uncertainty\nestimates to guide adaptive sampling. Using perceptual threshold data from four\nArgus II users, we show that GPR with a Mat\\'ern kernel provides more accurate\nthreshold predictions than a Radial Basis Function (RBF) kernel (p < .001,\nWilcoxon signed-rank test). In addition, spatially optimized sampling yielded\nlower prediction error than uniform random sampling for Participants 1 and 3 (p\n< .05). While adaptive sampling dynamically selects electrodes based on model\nuncertainty, its accuracy gains over spatial sampling were not statistically\nsignificant (p > .05), though it approached significance for Participant 1 (p =\n.074). These findings establish GPR with spatial sampling as a scalable,\nefficient approach to retinal prosthesis calibration, minimizing patient burden\nwhile maintaining predictive accuracy. More broadly, this framework offers a\ngeneralizable solution for adaptive calibration in neuroprosthetic devices with\nspatially structured stimulation thresholds.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.06672v1",
    "published_date": "2025-02-10T16:59:15Z",
    "keywords": []
  },
  {
    "title": "Identifying perturbation targets through causal differential networks",
    "authors": "Menghua Wu, Umesh Padia, Sean H. Murphy, Regina Barzilay, Tommi Jaakkola",
    "abstract": "  Identifying variables responsible for changes to a biological system enables\napplications in drug target discovery and cell engineering. Given a pair of\nobservational and interventional datasets, the goal is to isolate the subset of\nobserved variables that were the targets of the intervention. Directly applying\ncausal discovery algorithms is challenging: the data may contain thousands of\nvariables with as few as tens of samples per intervention, and biological\nsystems do not adhere to classical causality assumptions. We propose a\ncausality-inspired approach to address this practical setting. First, we infer\nnoisy causal graphs from the observational and interventional data. Then, we\nlearn to map the differences between these graphs, along with additional\nstatistical features, to sets of variables that were intervened upon. Both\nmodules are jointly trained in a supervised framework, on simulated and real\ndata that reflect the nature of biological interventions. This approach\nconsistently outperforms baselines for perturbation modeling on seven\nsingle-cell transcriptomics datasets. We also demonstrate significant\nimprovements over current causal discovery methods for predicting soft and hard\nintervention targets across a variety of synthetic data.\n",
    "pdf_url": "http://arxiv.org/pdf/2410.03380v2",
    "published_date": "2024-10-04T12:48:21Z",
    "keywords": []
  },
  {
    "title": "Decoding Gray Matter: large-scale analysis of brain cell morphometry to\n  inform microstructural modeling of diffusion MR signals",
    "authors": "Charlie Aird-Rossiter, Hui Zhang, Daniel C. Alexander, Derek K. Jones, Marco Palombo",
    "abstract": "  The structure of grey matter has long been a key focus in neuroscience, as\ncell morphology varies by type and can be affected by neurological conditions.\nUnderstanding these variations is essential for studying brain function and\ndisease. Diffusion-weighted MRI (dMRI) is a powerful non-invasive tool for\nexamining cellular microstructure in vivo. However, for dMRI to accurately\nreflect cellular features, it is crucial to determine which aspects of\nmorphology influence its measurements. Proper interpretation of dMRI data\ndepends on understanding its sensitivity to different cellular characteristics.\n  Despite growing interest in cellular morphology, there has been no systematic\nreport on the key features defining different neural cell types. To address\nthis, we analyzed over 3,500 three-dimensional cellular reconstructions across\nthree species and nine cell types, establishing reference values for critical\nmorphological traits. These traits fall into three categories: structural\nfeatures that define the cell's skeletal framework, shape features that\ndescribe spatial organization, and topological features that break down\ncellular structure to distinguish cell types. Beyond reporting these reference\nvalues, we examine their relevance for dMRI, identifying which neural features\ndMRI can detect and which cell types may be distinguishable.\n  This work provides essential benchmarks for grey matter research, offering\nnew guidelines on linking neuroimaging measurements to neurobiology. These\nreference values will be a valuable resource for neuroscientists and\nneuroimaging researchers, aiding in the interpretation of imaging data and the\nrefinement of brain tissue models.\n",
    "pdf_url": "http://arxiv.org/pdf/2501.02100v3",
    "published_date": "2025-01-03T21:01:05Z",
    "keywords": []
  },
  {
    "title": "SparseFocus: Learning-based One-shot Autofocus for Microscopy with\n  Sparse Content",
    "authors": "Yongping Zhai, Xiaoxi Fu, Qiang Su, Jia Hu, Yake Zhang, Yunfeng Zhou, Chaofan Zhang, Xiao Li, Wenxin Wang, Dongdong Wu, Shen Yan",
    "abstract": "  Autofocus is necessary for high-throughput and real-time scanning in\nmicroscopic imaging. Traditional methods rely on complex hardware or iterative\nhill-climbing algorithms. Recent learning-based approaches have demonstrated\nremarkable efficacy in a one-shot setting, avoiding hardware modifications or\niterative mechanical lens adjustments. However, in this paper, we highlight a\nsignificant challenge that the richness of image content can significantly\naffect autofocus performance. When the image content is sparse, previous\nautofocus methods, whether traditional climbing-hill or learning-based, tend to\nfail. To tackle this, we propose a content-importance-based solution, named\nSparseFocus, featuring a novel two-stage pipeline. The first stage measures the\nimportance of regions within the image, while the second stage calculates the\ndefocus distance from selected important regions. To validate our approach and\nbenefit the research community, we collect a large-scale dataset comprising\nmillions of labelled defocused images, encompassing both dense, sparse and\nextremely sparse scenarios. Experimental results show that SparseFocus\nsurpasses existing methods, effectively handling all levels of content\nsparsity. Moreover, we integrate SparseFocus into our Whole Slide Imaging (WSI)\nsystem that performs well in real-world applications. The code and dataset will\nbe made available upon the publication of this paper.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.06452v1",
    "published_date": "2025-02-10T13:31:32Z",
    "keywords": []
  },
  {
    "title": "A Simple yet Effective DDG Predictor is An Unsupervised Antibody\n  Optimizer and Explainer",
    "authors": "Lirong Wu, Yunfan Liu, Haitao Lin, Yufei Huang, Guojiang Zhao, Zhifeng Gao, Stan Z. Li",
    "abstract": "  The proteins that exist today have been optimized over billions of years of\nnatural evolution, during which nature creates random mutations and selects\nthem. The discovery of functionally promising mutations is challenged by the\nlimited evolutionary accessible regions, i.e., only a small region on the\nfitness landscape is beneficial. There have been numerous priors used to\nconstrain protein evolution to regions of landscapes with high-fitness\nvariants, among which the change in binding free energy (DDG) of protein\ncomplexes upon mutations is one of the most commonly used priors. However, the\nhuge mutation space poses two challenges: (1) how to improve the efficiency of\nDDG prediction for fast mutation screening; and (2) how to explain mutation\npreferences and efficiently explore accessible evolutionary regions. To address\nthese challenges, we propose a lightweight DDG predictor (Light-DDG), which\nadopts a structure-aware Transformer as the backbone and enhances it by\nknowledge distilled from existing powerful but computationally heavy DDG\npredictors. Additionally, we augmented, annotated, and released a large-scale\ndataset containing millions of mutation data for pre-training Light-DDG. We\nfind that such a simple yet effective Light-DDG can serve as a good\nunsupervised antibody optimizer and explainer. For the target antibody, we\npropose a novel Mutation Explainer to learn mutation preferences, which\naccounts for the marginal benefit of each mutation per residue. To further\nexplore accessible evolutionary regions, we conduct preference-guided antibody\noptimization and evaluate antibody candidates quickly using Light-DDG to\nidentify desirable mutations.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.06913v1",
    "published_date": "2025-02-10T09:26:57Z",
    "keywords": []
  },
  {
    "title": "High-Intensity Helical Flow: A Double-Edged Sword in Coronary Artery\n  Haemodynamics",
    "authors": "Chi Shen, Mingzi Zhang, Hamed Keramati, Diogo Almeida, Susann Beier",
    "abstract": "  The role of Helical Flow (HF) in human coronary arteries remains uncertain,\nyet its understanding promises unprecedented insights into atherosclerotic\nprocesses. In this study, we investigated the effects of HF and key\nhaemodynamic descriptors in 39 patient-specific left coronary artery trees from\nthe ASOCA dataset, including 20 non-stenosed and 19 stenosed cases. Absolute HF\nintensity $h_2$ correlated with higher Time-Averaged Endothelial Shear Stress\n(TAESS) in all vessel segments regardless of stenosis (p < 0.05). In stenosed\ncases, this correlation was so prominent that the vessel area exposed to\nadversely low TAESS was reduced (< 0.5 Pa, p = 0.0001), while areas of\nadversely high TAESS increased (> 4.71 Pa, p < 0.05), coinciding with high\n$h_2$ regions. This suggests that HF in coronary arteries is not always\nprotective as previously thought. It not only mitigates low TAESS, which is\nassociated with long-term plaque development and restenosis, but also\nexacerbates adversely high TAESS, which is linked to increased plaque\nvulnerability and acute events. Our findings redefine the current understanding\nof helical blood flow's role in cardiovascular atherosclerotic disease\nprocesses.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.06161v1",
    "published_date": "2025-02-10T05:20:41Z",
    "keywords": []
  },
  {
    "title": "A CT Geometry With Multiple Centers Of Rotation For Solving Sparse View\n  Problem",
    "authors": "Jiayu Duan, Yang Li, Jianmei Cai, Xuanqin Mou",
    "abstract": "  With the emergence of CNT (Carbon nanotube), static and instant CT scanning\nbecomes possible. By transforming the traditionally rotated thermal source into\na static ring array source composed of multiple CNTs, the imaging system can\nachieve high temporal resolution in scanning. However, due to the\nnon-negligible packaging size of CNTs, the static CT based on CNTs faces sparse\nview problem, which affects the image quality by introducing streak artifacts.\nIn this study, we based on the local correlation equation (LCE) to address the\nsparse view problem of static CT. The LCE is a series of partial differential\nequations (PDEs) to describe the local correlation of Radon transform in a\nneighborhood projection domain. Based on LCE, we analyze the characteristic of\nsparse view projection and propose a scanning geometry with multiple rotation\ncenters, which is different from existing CT devices that acquires the\nprojection around one rotation center. Specifically, in the proposed scanning\ngeometry, the circular ring array X-ray sources is divided into several arcs\nwhile the sources of each arc share one rotation center. All rotation centers\nof the arcs are uniformly distributed on a small circle. The optimal\ndistribution of the rotation centers can be optimized by the radius of the\ncircle. Moreover, to elevate the image quality under the sparse view\nreconstruction, we employed the LCE to interpolate unmeasured projections.\nCompared to the single rotation center scheme used in existing CT geometries,\nthe multiple rotation centers scan contributes to a more even projection\ndistribution with same view number. The simulated results demonstrated the\nefficiency and potential applications of the proposed method in static CT\nreconstructions.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.06125v1",
    "published_date": "2025-02-10T03:24:45Z",
    "keywords": []
  },
  {
    "title": "Neurophysiological correlates to the human brain complexity through\n  $q$-statistical analysis of electroencephalogram",
    "authors": "Dimitri Marques Abramov, Daniel de Freitas Quintanilha, Henrique Santos Lima, Roozemeria Pereira Costa, Carla Kamil-Leite, Vladimir V. Lazarev, Constantino Tsallis",
    "abstract": "  The prospects of assessing neural complexity (NC) by $q$-statistics of the\nsystemic organization of different types and levels of brain activity were\nstudied. In 70 adult subjects, NC was assessed via the parameter $q$ of\n$q$-statistics, applied to the ongoing and EEG and its spectral power of 20\nscalp points (channels). The NC were estimated both globally for all channels\n(AllCh) and locally (for each single channel) in different Functional States\n(FSs). The values of $q$ was compared among FSs and single channels, as well\nthey were correlated with the power of $\\theta$ (4-8Hz), $\\beta_1$ (15-25Hz)\nand others EEG bands, in each FS. The value of $q$ across all FSs was higher\nfor AllCh than for the single channels FSs. Consistently with previous studies,\nwe found a negative correlation between NC and age. The FSs did not influence\nthe $q$ of the EEG in AllCh, although locally the FS modulated $q$ in a\nconsistent manner (e.g., reducing $q$ in posterior sites with eyes closed). The\n$q$ was correlated positively with the power of the $\\theta$ and negatively\nwith that of the $\\beta_1$ band in general. These findings support the idea\nthat, as a first approach, $q$-statistics can describe the human NC. The\nrelationship between $q$ and $\\theta$ power aligns with greater NC during FSs\nsuch as listening music and resting with eyes open, which is consistent with\nhigh-order representations rather than low-informative attentional tasks\n(OddBall).\n",
    "pdf_url": "http://arxiv.org/pdf/2502.06057v1",
    "published_date": "2025-02-09T22:33:21Z",
    "keywords": []
  },
  {
    "title": "A coupled planar transmit RF array for ultrahigh field spine MR imaging",
    "authors": "Yunkun Zhao, Komlan Payne, Leslie Ying, Xiaoliang Zhang",
    "abstract": "  Ultrahigh-field MRI, such as those operating at 7 Tesla, enhances diagnostic\ncapabilities but also presents unique challenges, including the need for\nadvanced RF coil designs to achieve an optimal signal-to-noise ratio and\ntransmit efficiency, particularly when imaging large samples. In this work, we\nintroduce the coupled planar array, a novel technique for high-frequency,\nlarge-size RF coil design with enhanced the RF magnetic field (B1) efficiency\nand transmit performance for ultrahigh-field spine imaging applications. This\narray comprises multiple resonators that are electromagnetically coupled to\nfunction as a single multimodal resonator. The field distribution of its\nhighest frequency mode is suitable for spine imaging applications. Based on the\nnumerical modeling and calculation, a prototype of the coupled planar array was\nconstructed and its performance was evaluated through comprehensive numerical\nsimulations, rigorous RF measurements, empirical tests, and a comparison\nagainst a conventional surface coil with the same size and geometry. The\nresults of this study demonstrate that the proposed coupled planar array\nexhibits superior performance compared to conventional surface coils in terms\nof B1 efficiency for both transmit (B1+) and receive (B1-) fields, specific\nabsorption rate (SAR), and the ability to operate at high frequencies. This\nstudy suggests a promising and efficient approach to the design of\nhigh-frequency, large-size RF coils for spine MR imaging at ultrahigh magnetic\nfields.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.06041v1",
    "published_date": "2025-02-09T21:44:23Z",
    "keywords": []
  },
  {
    "title": "Deep Learning for Protein-Ligand Docking: Are We There Yet?",
    "authors": "Alex Morehead, Nabin Giri, Jian Liu, Pawan Neupane, Jianlin Cheng",
    "abstract": "  The effects of ligand binding on protein structures and their in vivo\nfunctions carry numerous implications for modern biomedical research and\nbiotechnology development efforts such as drug discovery. Although several deep\nlearning (DL) methods and benchmarks designed for protein-ligand docking have\nrecently been introduced, to date no prior works have systematically studied\nthe behavior of the latest docking and structure prediction methods within the\nbroadly applicable context of (1) using predicted (apo) protein structures for\ndocking (e.g., for applicability to new proteins); (2) binding multiple\n(cofactor) ligands concurrently to a given target protein (e.g., for enzyme\ndesign); and (3) having no prior knowledge of binding pockets (e.g., for\ngeneralization to unknown pockets). To enable a deeper understanding of docking\nmethods' real-world utility, we introduce PoseBench, the first comprehensive\nbenchmark for broadly applicable protein-ligand docking. PoseBench enables\nresearchers to rigorously and systematically evaluate DL methods for\napo-to-holo protein-ligand docking and protein-ligand structure prediction\nusing both primary ligand and multi-ligand benchmark datasets, the latter of\nwhich we introduce for the first time to the DL community. Empirically, using\nPoseBench, we find that (1) DL co-folding methods generally outperform\ncomparable conventional and DL docking baselines, yet popular methods such as\nAlphaFold 3 are still challenged by prediction targets with novel protein\nsequences; (2) certain DL co-folding methods are highly sensitive to their\ninput multiple sequence alignments, while others are not; and (3) DL methods\nstruggle to strike a balance between structural accuracy and chemical\nspecificity when predicting novel or multi-ligand protein targets. Code, data,\ntutorials, and benchmark results are available at\nhttps://github.com/BioinfoMachineLearning/PoseBench.\n",
    "pdf_url": "http://arxiv.org/pdf/2405.14108v5",
    "published_date": "2024-05-23T02:27:39Z",
    "keywords": []
  },
  {
    "title": "3D Single-shot CEST imaging at 3T Based on True FISP Readout",
    "authors": "Yupeng Wu, Qifan Pang, Zhichao Wang, Gaiying Li, Caixia Fu, Mengqiu Cao, Xingrui Wang, Yang Song, Yu Zhao, Jianqi Li",
    "abstract": "  To simultaneously fit multiple-pool effects, spectrally selective 3D CEST\nimaging typically requires single-shot readouts to save time. However, to date,\nFLASH and EPI have been the primary pulse sequences used for this purpose. They\nsuffer from low SNR or image distortion related to B0 field inhomogeneity. In\nthis work, we developed a 3D single-shot CEST sequence using true fast imaging\nwith steady-state precession (True FISP) readout, also known as bSSFP, and\noptimized the scanning parameters through simulations. The performance of the\nCEST sequence was validated using an egg white phantom, ten healthy volunteers,\nand a patient with a brain tumor on a 3T human scanner. Subsequently, the\nproposed CEST sequence using True FISP was compared with the commonly used\nFLASH-based CEST sequence, focusing on SNR and image contrast, while\nmaintaining identical pre-saturation modes, repetition time, echo time and scan\ntime. In the simulation experiments, the maximum CEST signal obtained from the\nTrue FISP was significantly greater than that obtained from the FLASH sequence.\nIn the egg white phantom, the SNRs of amide proton transfer (APT) and nuclear\nOverhauser enhancement (NOE) effect images obtained from the True FISP were\n68.3% and 57.0% higher than those obtained from the FLASH sequence,\nrespectively. In healthy volunteers, saturated images collected with the True\nFISP sequence at 3.5 ppm showed an approximate 84% increase in mean temporal\nSNR compared to those collected with the FLASH sequence. Compared to the FLASH\nsequence, the CEST images obtained from the True FISP sequence could display\nmore detailed brain tissue structures of both normal individuals and the\npatient with a brain tumor. Therefore, due to the high SNR inherent in the\nsequence, True FISP has the potential to be used for fast and high-quality 3D\nimage readout of CEST contrasts in clinical applications.\n",
    "pdf_url": "http://arxiv.org/pdf/2501.03548v2",
    "published_date": "2025-01-07T05:48:08Z",
    "keywords": []
  },
  {
    "title": "LLMs for Drug-Drug Interaction Prediction: A Comprehensive Comparison",
    "authors": "Gabriele De Vito, Filomena Ferrucci, Athanasios Angelakis",
    "abstract": "  The increasing volume of drug combinations in modern therapeutic regimens\nneeds reliable methods for predicting drug-drug interactions (DDIs). While\nLarge Language Models (LLMs) have revolutionized various domains, their\npotential in pharmaceutical research, particularly in DDI prediction, remains\nlargely unexplored. This study thoroughly investigates LLMs' capabilities in\npredicting DDIs by uniquely processing molecular structures (SMILES), target\norganisms, and gene interaction data as raw text input from the latest DrugBank\ndataset. We evaluated 18 different LLMs, including proprietary models (GPT-4,\nClaude, Gemini) and open-source variants (from 1.5B to 72B parameters), first\nassessing their zero-shot capabilities in DDI prediction. We then fine-tuned\nselected models (GPT-4, Phi-3.5 2.7B, Qwen-2.5 3B, Gemma-2 9B, and Deepseek R1\ndistilled Qwen 1.5B) to optimize their performance. Our comprehensive\nevaluation framework included validation across 13 external DDI datasets,\ncomparing against traditional approaches such as l2-regularized logistic\nregression. Fine-tuned LLMs demonstrated superior performance, with Phi-3.5\n2.7B achieving a sensitivity of 0.978 in DDI prediction, with an accuracy of\n0.919 on balanced datasets (50% positive, 50% negative cases). This result\nrepresents an improvement over both zero-shot predictions and state-of-the-art\nmachine-learning methods used for DDI prediction. Our analysis reveals that\nLLMs can effectively capture complex molecular interaction patterns and cases\nwhere drug pairs target common genes, making them valuable tools for practical\napplications in pharmaceutical research and clinical settings.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.06890v1",
    "published_date": "2025-02-09T09:58:12Z",
    "keywords": []
  },
  {
    "title": "CISCA and CytoDArk0: a Cell Instance Segmentation and Classification\n  method for histo(patho)logical image Analyses and a new, open, Nissl-stained\n  dataset for brain cytoarchitecture studies",
    "authors": "Valentina Vadori, Jean-Marie Graïc, Antonella Peruffo, Giulia Vadori, Livio Finos, Enrico Grisan",
    "abstract": "  Delineating and classifying individual cells in microscopy tissue images is\ninherently challenging yet remains essential for advancements in medical and\nneuroscientific research. In this work, we propose a new deep learning\nframework, CISCA, for automatic cell instance segmentation and classification\nin histological slices. At the core of CISCA is a network architecture\nfeaturing a lightweight U-Net with three heads in the decoder. The first head\nclassifies pixels into boundaries between neighboring cells, cell bodies, and\nbackground, while the second head regresses four distance maps along four\ndirections. The outputs from the first and second heads are integrated through\na tailored post-processing step, which ultimately produces the segmentation of\nindividual cells. The third head enables the simultaneous classification of\ncells into relevant classes, if required. We demonstrate the effectiveness of\nour method using four datasets, including CoNIC, PanNuke, and MoNuSeg, which\nare publicly available H&Estained datasets that cover diverse tissue types and\nmagnifications. In addition, we introduce CytoDArk0, the first annotated\ndataset of Nissl-stained histological images of the mammalian brain, containing\nnearly 40k annotated neurons and glia cells, aimed at facilitating advancements\nin digital neuropathology and brain cytoarchitecture studies. We evaluate CISCA\nagainst other state-of-the-art methods, demonstrating its versatility,\nrobustness, and accuracy in segmenting and classifying cells across diverse\ntissue types, magnifications, and staining techniques. This makes CISCA\nwell-suited for detailed analyses of cell morphology and efficient cell\ncounting in both digital pathology workflows and brain cytoarchitecture\nresearch.\n",
    "pdf_url": "http://arxiv.org/pdf/2409.04175v2",
    "published_date": "2024-09-06T10:34:06Z",
    "keywords": []
  },
  {
    "title": "Automated Cell Structure Extraction for 3D Electron Microscopy by Deep\n  Learning",
    "authors": "Jin Kousaka, Atsuko H. Iwane, Yuichi Togashi",
    "abstract": "  Modeling the 3D structures of cells and tissues is crucial in biology.\nSequential cross-sectional images from electron microscopy provide\nhigh-resolution intracellular structure information. The segmentation of\ncomplex cell structures remains a laborious manual task for experts, demanding\ntime and effort. This bottleneck in analyzing biological images requires\nefficient and automated solutions. In this study, the deep learning-based\nautomated segmentation of biological images was explored to enable accurate\nreconstruction of the 3D structures of cells and organelles. An analysis system\nfor the cell images of Cyanidioschyzon merolae, a primitive unicellular red\nalgae, was constructed. This system utilizes sequential cross-sectional images\ncaptured by a focused ion beam scanning electron microscope (FIB-SEM). A U-Net\nwas adopted and training was performed to identify and segment cell organelles\nfrom single-cell images. In addition, the segment anything model (SAM) and 3D\nwatershed algorithm were employed to extract individual 3D images of each cell\nfrom large-scale microscope images containing numerous cells. Finally, the\ntrained U-Net was applied to segment each structure within these 3D images.\nThrough this procedure, the creation of 3D cell models could be fully\nautomated. The adoption of other deep learning techniques and combinations of\nimage processing methods will also be explored to enhance the segmentation\naccuracy further.\n",
    "pdf_url": "http://arxiv.org/pdf/2405.06303v3",
    "published_date": "2024-05-10T08:16:41Z",
    "keywords": []
  },
  {
    "title": "FRAP analysis Measuring biophysical kinetic parameters using image\n  analysis",
    "authors": "Sharva V. Hiremath, Etika Goyal, Gregory T. Reeves, Cranos M. Williams",
    "abstract": "  Understanding transcription factor dynamics is crucial for unraveling the\nregulatory mechanisms of gene expression that underpin cellular function and\ndevelopment. Measurements of transcription factor subcellular movements are\nessential for developing predictive models of gene expression. However,\nobtaining these quantitative measurements poses significant challenges due to\nthe inherent variability of biological data and the need for high precision in\ntracking the movement and interaction of molecules. Our computational pipeline\nprovides a solution to these challenges, offering a comprehensive approach to\nthe quantitative analysis of transcription factor dynamics. Our pipeline\nintegrates advanced image segmentation to accurately delineate individual\nnuclei, precise nucleus tracking to monitor changes over time, and detailed\nintensity extraction to measure fluorescence as a proxy for transcription\nfactor activity. Combining our pipeline with techniques such as fluorescence\nrecovery after photobleaching enables the estimation of vital biophysical\nparameters, such as transcription factor import and export rates.\n",
    "pdf_url": "http://arxiv.org/pdf/2402.16615v3",
    "published_date": "2024-02-23T17:57:32Z",
    "keywords": []
  },
  {
    "title": "Magnetic Resonance Image Processing Transformer for General Accelerated\n  Image Reconstruction",
    "authors": "Guoyao Shen, Mengyu Li, Stephan Anderson, Chad W. Farris, Xin Zhang",
    "abstract": "  Recent advancements in deep learning have enabled the development of\ngeneralizable models that achieve state-of-the-art performance across various\nimaging tasks. Vision Transformer (ViT)-based architectures, in particular,\nhave demonstrated strong feature extraction capabilities when pre-trained on\nlarge-scale datasets. In this work, we introduce the Magnetic Resonance Image\nProcessing Transformer (MR-IPT), a ViT-based framework designed to enhance the\ngeneralizability and robustness of accelerated MRI reconstruction. Unlike\nconventional deep learning models that require separate training for different\nacceleration factors, MR-IPT is pre-trained on a large-scale dataset\nencompassing multiple undersampling patterns and acceleration settings,\nenabling a unified reconstruction framework. By leveraging a shared transformer\nbackbone, MR-IPT effectively learns universal feature representations, allowing\nit to generalize across diverse reconstruction tasks. Extensive experiments\ndemonstrate that MR-IPT outperforms both CNN-based and existing\ntransformer-based methods, achieving superior reconstruction quality across\nvarying acceleration factors and sampling masks. Moreover, MR-IPT exhibits\nstrong robustness, maintaining high performance even under unseen acquisition\nsetups, highlighting its potential as a scalable and efficient solution for\naccelerated MRI. Our findings suggest that transformer-based general models can\nsignificantly advance MRI reconstruction, offering improved adaptability and\nstability compared to traditional deep learning approaches.\n",
    "pdf_url": "http://arxiv.org/pdf/2405.15098v2",
    "published_date": "2024-05-23T23:13:02Z",
    "keywords": []
  },
  {
    "title": "A self-contact electromechanical framework for intestinal motility",
    "authors": "René Thierry Djoumessi, Pietro Lenarda, Alessio Gizzi, Marco Paggi",
    "abstract": "  This study introduces an advanced multiphysics and multiscale modeling\napproach to investigate intestinal motility. We propose a generalized\nelectromechanical framework that incorporates contact mechanics, enabling the\ndevelopment of a unique and innovative model for intestinal motility. The\ntheoretical framework includes an electromechanical model coupling a\nmicrostructural material model, which describes the intestinal structure, with\nan electrophysiological model that captures the propagation of slow waves.\nAdditionally, it integrates a self-contact detection algorithm based on a\nnearest-neighbour search and the penalty method, along with boundary conditions\nthat account for the influence of surrounding organs. A staggered finite\nelement scheme implemented in FEniCS is employed to solve the governing\nequations using the finite element method. The model is applied to study cases\nof moderate and severe strangulation hernia, as well as intestinal adhesion\nsyndrome. The results demonstrate that low peristalsis takes place in the\npre-strangulation zone. At the same time, very high pressure is recorded in the\nstrangulation zone, and peristaltic contractions persisted in the healthy\nregion. For adhesions, the results indicate a complete absence of peristalsis\nin the adherent region. The model successfully reproduces both qualitatively\nand quantitatively propagative contractions in complex scenarios, such as pre-\nand post-surgical conditions, thereby highlighting its potential to provide\nvaluable insights for clinical applications.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.05285v1",
    "published_date": "2025-02-07T19:42:51Z",
    "keywords": []
  },
  {
    "title": "Evaluation of Deep Learning-based Scatter Correction on a Long-axial\n  Field-of-view PET scanner",
    "authors": "Baptiste Laurent, Alexandre Bousse, Thibaut Merlin, Axel Rominger, Kuangyu Shi, Dimitris Visvikis",
    "abstract": "  Objective: Long-axial field-of-view (LAFOV) positron emission tomography\n(PET) systems allow higher sensitivity, with an increased number of detected\nlines of response induced by a larger angle of acceptance. However, this\nextended angle increases the number of multiple scatters and the scatter\ncontribution within oblique planes. As scattering affects both quality and\nquantification of the reconstructed image, it is crucial to correct this effect\nwith more accurate methods than the state-of-the-art single scatter simulation\n(SSS) that can reach its limits with such an extended field-of-view (FOV). In\nthis work, which is an extension of our previous assessment of deep\nlearning-based scatter estimation (DLSE) carried out on a conventional PET\nsystem, we aim to evaluate the DLSE method performance on LAFOV total-body PET.\n  Approach: The proposed DLSE method based on a convolutional neural network\n(CNN) U-Net architecture uses emission and attenuation sinograms to estimate\nscatter sinogram. The network was trained from Monte-Carlo (MC) simulations of\nXCAT phantoms [18F]-FDG PET acquisitions using a Siemens Biograph Vision Quadra\nscanner model, with multiple morphologies and dose distributions. We firstly\nevaluated the method performance on simulated data in both sinogram and image\ndomain by comparing it to the MC ground truth and SSS scatter sinograms. We\nthen tested the method on seven [18F]-FDG and seven [18F]-PSMA clinical\ndatasets, and compare it to SSS estimations.\n  Results: DLSE showed superior accuracy on phantom data, greater robustness to\npatient size and dose variations compared to SSS, and better lesion contrast\nrecovery. It also yielded promising clinical results, improving lesion\ncontrasts in [18F]-FDG datasets and performing consistently with [18F]-PSMA\ndatasets despite no training with [18F]-PSMA.\n",
    "pdf_url": "http://arxiv.org/pdf/2501.01341v4",
    "published_date": "2025-01-02T16:47:09Z",
    "keywords": []
  },
  {
    "title": "Calibration of a $Δ$E-E telescope based on CeBr$_3$ scintillator\n  for secondary charged particles measurements in hadron therapy",
    "authors": "L. Gesson, J. Gross, C. Mozzi, C. Reibel, Ch. Finck, S. Higueret, T. D. Le, E. Traykov, J. C. Thomas, N. Arbor, M. Pullia, G. Harmant, M. Vanstalle",
    "abstract": "  Hadrontherapy is a promising cancer treatment method that offers better dose\nconformity and reduces damage to healthy tissues compared to conventional\nradiotherapy. However, one major challenge remaining is the precise\ncharacterization of secondary particles generated by nuclear interactions of\nthe primary beam with tissues. Current data on secondary charged particles,\nsuch as protons and light ions, remain insufficient, particularly in the\nclinically relevant energy ranges. This lack of experimental data introduces\nuncertainties in treatment planning softwares and Monte Carlo calculations,\nthus compromising the accuracy of dose delivery to the patients. This work\nconsists in the characterization of secondary charged particles generated in\nhadron therapy using a $\\Delta$E-E telescope comprising a CeBr$_3$ crystal\nscintillator and a plastic scintillator. The calibration and response of this\ntelescope to ions commonly used in clinical settings is presented in this work,\nhighlighting adherence to Birks law for accurate energy measurements. This\nstudy is the first to optimize a $\\Delta$E-E telescope combining CeBr$_3$ and\nplastic scintillators specifically for secondary particle detection in\nhadrontherapy. This represents an important step in the exploitation of the\nsystem for nuclear data acquisition, as it enables both the measurement of\nenergy and the discrimination of secondary particles. The objective is to\ndevelop a system compatible with clinical use, allowing for the most precise\npossible comparison with treatment planning software calculations.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.05050v1",
    "published_date": "2025-02-07T16:14:43Z",
    "keywords": [
      "cancer"
    ]
  },
  {
    "title": "R3F: An R package for evolutionary dates, rates, and priors using the\n  relative rate framework",
    "authors": "Qiqing Tao, Sudip Sharma, Koichiro Tamura, Sudhir Kumar",
    "abstract": "  The relative rate framework (RRF) can estimate divergence times from branch\nlengths in a phylogeny, which is the theoretical basis of the RelTime method\nfrequently applied, a relaxed clock approach for molecular dating that scales\nwell for large phylogenies. The use of RRF has also enabled the development of\ncomputationally efficient and accurate methods for testing the autocorrelation\nof lineage rates in a phylogeny (CorrTest) and selecting data-driven parameters\nof the birth-death speciation model (ddBD), which can be used to specify priors\nin Bayesian molecular dating. We have developed R3F, an R package implementing\nRRF to estimate divergence times, infer lineage rates, conduct CorrTest, and\nbuild a ddBD tree prior for Bayesian dating in molecular phylogenies. Here, we\ndescribe R3F functionality and explain how to interpret and use its outputs in\nother visualization software and packages, such as MEGA, ggtree, and FigTree.\nUltimately, R3F is intended to enable the dating of the Tree of Life with\ngreater accuracy and precision, which would have important implications for\nstudies of organism evolution, diversification dynamics, phylogeography, and\nbiogeography. Availability and Implementation: The source codes and related\ninstructions for installing and implementing R3F are available from GitHub\n(https://github.com/cathyqqtao/R3F).\n",
    "pdf_url": "http://arxiv.org/pdf/2502.05004v1",
    "published_date": "2025-02-07T15:24:19Z",
    "keywords": []
  },
  {
    "title": "Enhancing kidney quality assessment: Power Doppler during normothermic\n  machine perfusion",
    "authors": "Yitian Fang, Anton V. Nikolaev, Jeroen Essers, Gisela Ambagtsheer, Marian C. Clahsen-van Groningen, Robert C. Minnee, Ron W. F. de Bruin, Gijs van Soest",
    "abstract": "  Objectives: Marginal donor kidneys are increasingly used for transplantation\nto overcome organ shortage. This study aims to investigate the additional value\nof Power Doppler (PD) imaging in kidney quality assessment during normothermic\nmachine perfusion (NMP). Methods: Porcine kidneys (n=22) retrieved from a local\nslaughterhouse underwent 2 hours of NMP. Based on creatinine clearance (CrCl)\nand oxygen consumption (VO2), the kidneys were classified as functional (n=7)\nand non-functional (n=15) kidneys. PD imaging was performed at 30, 60, and 120\nminutes, and PD metrics, including vascularization index (VI), flow index (FI),\nand vascularization flow index (VFI) were calculated. Renal blood flow (RBF),\nCrCl, and VO2 were measured at the same time points during NMP. The metrics\nwere compared utilizing correlation analysis. Results: FI and VFI moderately\ncorrelated with CrCl (r=0.537, p<0.0001; r=0.536, p<0.0001, respectively),\nwhile VI strongly correlated with VO2 (r=0.839, p<0.0001). At 120 minutes, PD\nmetrics demonstrated the highest diagnostic accuracy for distinguishing\nfunctional from non-functional kidneys, with an area under the curve (AUC) of\n0.943 for VI, 0.924 for FI, and 0.943 for VFI. Cutoff values of 17% for VI, 50\na.u. for FI, and 9 a.u. for VFI provided 100% specificity and 73% sensitivity\nto identify non-functional kidneys, with an overall diagnostic accuracy of 82%.\nBaseline kidney biopsies showed moderate acute tubular necrosis in both groups\nwith no significant differences. Conclusions: PD metrics strongly correlate\nwith renal viability and effectively differentiate functional from\nnon-functional kidneys. PD imaging can be a valuable alternative to RBF during\nNMP for kidney assessment.\n",
    "pdf_url": "http://arxiv.org/pdf/2501.04457v2",
    "published_date": "2025-01-08T12:25:33Z",
    "keywords": []
  },
  {
    "title": "MultistageOT: Multistage optimal transport infers trajectories from a\n  snapshot of single-cell data",
    "authors": "Magnus Tronstad, Johan Karlsson, Joakim S. Dahlin",
    "abstract": "  Single-cell RNA-sequencing captures a temporal slice, or a snapshot, of a\ncell differentiation process. A major bioinformatical challenge is the\ninference of differentiation trajectories from a single snapshot, and methods\nthat account for outlier cells that are unrelated to the differentiation\nprocess have yet to be established. We present MultistageOT: a Multistage\nOptimal Transport-based framework for trajectory inference in a snapshot\n(https://github.com/dahlinlab/MultistageOT). Application of optimal transport\nhas proven successful for many single-cell tasks, but classical bimarginal\noptimal transport for trajectory inference fails to model temporal progression\nin a snapshot. Representing a novel generalization of optimal transport,\nMultistageOT addresses this major limitation by introducing a temporal\ndimension, allowing for high resolution modeling of intermediate\ndifferentiation stages. We challenge MultistageOT with snapshot data of cell\ndifferentiation, demonstrating effectiveness in pseudotime ordering, detection\nof outliers, and significantly improved fate prediction accuracy over\nstate-of-the-art.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.05241v1",
    "published_date": "2025-02-07T12:28:25Z",
    "keywords": []
  },
  {
    "title": "Bioregionalization analyses with the bioregion R-package",
    "authors": "Pierre Denelle, Boris Leroy, Maxime Lenormand",
    "abstract": "  Bioregionalization consists in the identification of spatial units with\nsimilar species composition and is a classical approach in the fields of\nbiogeography and macroecology. The recent emergence of global databases,\nimprovements in computational power, and the development of clustering\nalgorithms coming from the network theory have led to several major updates of\nthe bioregionalizations of many taxa. A typical bioregionalization workflow\ninvolves five different steps: formatting the input data, computing a\n(dis)similarity matrix, selecting a bioregionalization algorithm, evaluating\nthe resulting bioregionalization, and mapping and interpreting the bioregions.\nFor most of these steps, there are many options available in the methods and R\npackages. Here, we present bioregion, a package that includes all the steps of\na bioregionalization workflow under a single architecture, with an exhaustive\nlist of the bioregionalization algorithms used in biogeography and\nmacroecology. These algorithms include (non-)hierarchical algorithms as well as\ncommunity detection algorithms coming from the network theory. Some key methods\nfrom the literature, such as the network community detection algorithm Infomap\nor OSLOM (Order Statistics Local Optimization Method), that were not available\nin the R language are included in bioregion. By combining different methods\ncoming from different fields to communicate easily, bioregion will allow a\nreproducible and complete comparison of the different bioregionalization\nmethods, which is still missing in the literature.\n",
    "pdf_url": "http://arxiv.org/pdf/2404.15300v2",
    "published_date": "2024-03-28T08:47:32Z",
    "keywords": []
  },
  {
    "title": "LUND-PROBE -- LUND Prostate Radiotherapy Open Benchmarking and\n  Evaluation dataset",
    "authors": "Viktor Rogowski, Lars E Olsson, Jonas Scherman, Emilia Persson, Mustafa Kadhim, Sacha af Wetterstedt, Adalsteinn Gunnlaugsson, Martin P. Nilsson, Nandor Vass, Mathieu Moreau, Maria Gebre Medhin, Sven Bäck, Per Munck af Rosenschöld, Silke Engelholm, Christian Jamtheim Gustafsson",
    "abstract": "  Radiotherapy treatment for prostate cancer relies on computed tomography (CT)\nand/or magnetic resonance imaging (MRI) for segmentation of target volumes and\norgans at risk (OARs). Manual segmentation of these volumes is regarded as the\ngold standard for ground truth in machine learning applications but to acquire\nsuch data is tedious and time-consuming. A publicly available clinical dataset\nis presented, comprising MRI- and synthetic CT (sCT) images, target and OARs\nsegmentations, and radiotherapy dose distributions for 432 prostate cancer\npatients treated with MRI-guided radiotherapy. An extended dataset with 35\npatients is also included, with the addition of deep learning (DL)-generated\nsegmentations, DL segmentation uncertainty maps, and DL segmentations manually\nadjusted by four radiation oncologists. The publication of these resources aims\nto aid research within the fields of automated radiotherapy treatment planning,\nsegmentation, inter-observer analyses, and DL model uncertainty investigation.\nThe dataset is hosted on the AIDA Data Hub and offers a free-to-use resource\nfor the scientific community, valuable for the advancement of medical imaging\nand prostate cancer radiotherapy research.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.04493v1",
    "published_date": "2025-02-06T20:44:42Z",
    "keywords": [
      "cancer"
    ]
  },
  {
    "title": "DiffNMR2: NMR Guided Sampling Acquisition Through Diffusion Model\n  Uncertainty",
    "authors": "Etienne Goffinet, Sen Yan, Fabrizio Gabellieri, Laurence Jennings, Lydia Gkoura, Filippo Castiglione, Ryan Young, Idir Malki, Ankita Singh, Thomas Launey",
    "abstract": "  Nuclear Magnetic Resonance (NMR) spectrometry uses electro-frequency pulses\nto probe the resonance of a compound's nucleus, which is then analyzed to\ndetermine its structure. The acquisition time of high-resolution NMR spectra\nremains a significant bottleneck, especially for complex biological samples\nsuch as proteins. In this study, we propose a novel and efficient sub-sampling\nstrategy based on a diffusion model trained on protein NMR data. Our method\niteratively reconstructs under-sampled spectra while using model uncertainty to\nguide subsequent sampling, significantly reducing acquisition time. Compared to\nstate-of-the-art strategies, our approach improves reconstruction accuracy by\n52.9\\%, reduces hallucinated peaks by 55.6%, and requires 60% less time in\ncomplex NMR experiments. This advancement holds promise for many applications,\nfrom drug discovery to materials science, where rapid and high-resolution\nspectral analysis is critical.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.05230v1",
    "published_date": "2025-02-06T20:10:28Z",
    "keywords": []
  },
  {
    "title": "Cryogenic cesium iodide as a potential PET material",
    "authors": "S. R. Soleti, A. Castillo, M. del Barrio-Torregrosa, C. Echeverria, M. Seemann, D. Zerzion, J. J. Gómez Cadenas",
    "abstract": "  Total Body PET (TBPET) scanners have recently demonstrated the ability to\nsignificantly reduce both acquisition time and the administered radioactive\ndose, thanks to their increased sensitivity. However, their widespread adoption\nis limited by the high costs associated with the current available systems.\nCesium iodide (CsI), though historically less favored for PET due to its lower\nstopping power and light yield compared to crystals like LYSO, shows remarkable\nimprovement when operated at cryogenic temperatures ($\\sim$100 K). Under these\nconditions, CsI light yield rises dramatically to about 100 photons/keV,\nproviding excellent energy resolution and good coincidence time resolution at a\nlower cost - typically 3 to 5 times cheaper than other crystals at parity of\nradiation length. In our study, we measured the light yield, the energy\nresolution and the coincidence time resolution as a function of temperature for\ntwo pure CsI crystals read out by a pair of silicon photomultipliers (SiPMs).\nAn energy resolution of 6.3% FWHM and a coincidence time resolution of 1.84 ns\nat 511 keV were achieved at a temperature of 104 K. These results point towards\nthe potential of cryogenic CsI as a cost-effective, high-performance material\nfor TBPET scanners.\n",
    "pdf_url": "http://arxiv.org/pdf/2406.13598v2",
    "published_date": "2024-06-19T14:41:21Z",
    "keywords": []
  },
  {
    "title": "Revisiting convolutive blind source separation for identifying spiking\n  motor neuron activity: From theory to practice",
    "authors": "Thomas Klotz, Robin Rohlén",
    "abstract": "  Objective: Identifying the activity of motor neurons (MNs) non-invasively is\npossible by decomposing signals from muscles, e.g., surface electromyography\n(EMG) or ultrasound. The theoretical background of MN identification is\nconvolutive blind source separation (cBSS), and different algorithms have been\ndeveloped and validated. Yet, the existence and identifiability of inverse\nsolutions and the corresponding estimation errors are not fully understood.\nFurther, the guidelines for selecting appropriate parameters are often built on\nempirical observations, limiting the translation to clinical applications and\nother modalities. Approach: We revisited the cBSS model for MN identification,\naugmented it with new theoretical insights and derived a framework that can\npredict the existence of inverse solutions. This framework allows the\nquantification of estimation errors due to the imperfect inversion of the motor\nunit action potentials (MUAP), noise sources, and the ill-conditioning of the\ninverse problem. To bridge the gap between theory and practice, we used\ncomputer simulations. Main results: (1) Increasing the similarity of MUAPs or\ncorrelation between spike trains increases the bias for detecting high\namplitude MUs. (2) The optimal objective function depends on the expected spike\namplitude, spike amplitude statistics and the amplitude of background spikes.\n(3) There is some wiggle room for MN detection given non-stationary MUAPs. (4)\nThere is no connection between MUAP duration and extension factor, in contrast\nto previous guidelines. (5) Source quality metrics like the silhouette score\n(SIL) or the pulse-to-noise ratio (PNR) are highly correlated with a source's\nobjective function output. (6) SIL is superior to PNR. Significance: These\nfindings will guide cBSS algorithm developments tailored to MN identification\nand clinical application translation.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.04065v1",
    "published_date": "2025-02-06T13:23:30Z",
    "keywords": []
  },
  {
    "title": "A Cloud-native Agile approach to cyber platform prototyping and\n  integration for astronomy: the ENGAGE SKA case",
    "authors": "Domingos Barbosa, Diogo Regateiro, João Paulo Barraca, Dzianis Bartashevich, Marco Bartolini, Matteo di Carlo, Piers Harding, Dalmiro Maia, Bruno Morgado, Domingos Nunes, Bruno Ribeiro, Bruno Coelho, Valério Ribeiro, Allan K. de Almeida Jr, Timothée Vaillant, Uğur Yilmaz",
    "abstract": "  The Square Kilometre Array (SKA) Observatory is gearing up the formal\nconstruction of its two radio interferometers in Australia and South Africa\nafter the end of design and pre-construction phases. Agile methodologies, the\nCloud native Computing technologies and the DevOps software ideas are\ninfluencing the design of compute infrastructures that will be key to reduce\nthe operational costs of SKA while improving the control and monitoring of the\nSKA antennas and ancillary systems, Correlators, HPC facilities or related data\ncentre tiered systems. These tools will likely include advanced power metering\ntechnologies and efficient distribution automation and Network Operation\nCentres (NOC). SKA will become the world's largest radio telescope and is\nexpected to achieve its first science by 2026. To cope with this dimension and\ncomplexity, a key part of this distributed Observatory is the overall software\ncontrol and monitoring system embodied in the Observatory Management and\nControl (OMC) and the Services Teams that requires specialized Agile Teams to\nassist in software and cyber infrastructure building using an Agile development\nenvironment that includes test automation, Continuous Integration, and\nContinuous Deployment. To manage such a large and distributed machine, the\nAgile approach was adopted for the core software package of the SKA Telescope\naimed at scheduling observations, controlling their execution, monitoring the\ntelescope status and ensuring scalability and reliability. Here, we report on\nthe ENGAGE SKA ciberinfrastructure prototyping support to the SKA Agile\nSoftware Development Life Cycle (SDLC).\n",
    "pdf_url": "http://arxiv.org/pdf/2502.04039v1",
    "published_date": "2025-02-06T13:01:27Z",
    "keywords": []
  },
  {
    "title": "Effects of skull properties on long-pulsed transcranial focused\n  ultrasound transmission",
    "authors": "Han Li, Isla Barnard, Tyler Halliwell, Xinyu Zhang, Andreas Melzer, Zhihong Huang",
    "abstract": "  Transcranial low-intensity focused ultrasound can deliver energy to the brain\nin a minimally invasive manner for neuromodulation applications. However,\ncontinuous sonication through the skull introduces significant wave\ninteractions, complicating precise energy delivery to the target. We present a\ncomprehensive examination of intracranial acoustic fields generated by focused\nultrasound transducers and assess the characteristics of cranial bone that\naffect acoustic transmission. Acoustic field maps were generated at 88 regions\nof interest across 10 historical and 2 Thiel-embalmed human skull specimens\nwith sonication at frequencies of 220 kHz, 650 kHz, and 1000 kHz. The average\npeak pressure insertion losses for historical were 3.6$\\pm$3.4 dB, 9.3$\\pm$3.3\ndB, and 14.8$\\pm$5.8 dB, respectively, and for Thiel skulls, the respective\nlosses were 2.9$\\pm$1.8 dB, 9.4$\\pm$2.6 dB, and 17.0$\\pm$5.5 dB. The effect of\nskull thickness, skull density ratio, and skull curvature on intracranial peak\npressure, power and focal area was investigated and linear fits produced.\nSeveral unfavorable focusing performances were observed in regions with\nexcessive thickness variation. The effects of angulation and spacing between\nthe transducer and the skull were also investigated. Preliminary findings\nindicate that wave superposition resulting from skull and transducer spacing\ncould lead to a 30-40% uncertainty in peak recorded intracranial pressure.\n",
    "pdf_url": "http://arxiv.org/pdf/2405.08489v5",
    "published_date": "2024-05-14T10:29:20Z",
    "keywords": []
  },
  {
    "title": "A necessary condition for the guarantee of the superiorization method",
    "authors": "Kay Barshad, Yair Censor, Walaa Moursi, Tyler Weames, Henry Wolkowicz",
    "abstract": "  We study a method that involves principally convex feasibility-seeking and\nmakes secondary efforts of objective function value reduction. This is the\nwell-known superiorization method (SM), where the iterates of an asymptotically\nconvergent iterative feasibility-seeking algorithm are perturbed by objective\nfunction nonascent steps. We investigate the question under what conditions a\nsequence generated by an SM algorithm asymptotically converges to a feasible\npoint whose objective function value is superior (meaning smaller or equal) to\nthat of a feasible point reached by the corresponding unperturbed one (i.e.,\nthe exactly same feasibility-seeking algorithm that the SM algorithm employs.)\nThis question is yet only partially answered in the literature. We present a\ncondition under which an SM algorithm that uses negative gradient descent steps\nin its perturbations fails to yield such a superior outcome. The significance\nof the discovery of this negative condition is that it necessitates that the\ninverse of this condition will have to be assumed to hold in any future\nguarantee result for the SM. The condition is important for practitioners who\nuse the SM because it is avoidable in experimental work with the SM, thus\nincreasing the success rate of the method in real-world applications.\n",
    "pdf_url": "http://arxiv.org/pdf/2502.03867v1",
    "published_date": "2025-02-06T08:28:41Z",
    "keywords": []
  }
]